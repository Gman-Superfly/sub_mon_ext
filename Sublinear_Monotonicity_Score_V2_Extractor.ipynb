{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USFntMArrscw",
        "outputId": "58650796-8332-47d3-d0b5-683ea6859984"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing Monotonicity Score Function:\n",
            "\n",
            "Monotone Sequence:\n",
            "Score: 1.000\n",
            "Details: {'samples': 139, 'violations': 0, 'checks': 9591, 'epsilon': 0.1, 'sequence_length': 1000}\n",
            "\n",
            "Random Sequence:\n",
            "Score: 0.544\n",
            "Details: {'samples': 139, 'violations': 4370, 'checks': 9591, 'epsilon': 0.1, 'sequence_length': 1000}\n",
            "\n",
            "Decreasing Sequence:\n",
            "Score: 0.000\n",
            "Details: {'samples': 139, 'violations': 9591, 'checks': 9591, 'epsilon': 0.1, 'sequence_length': 1001}\n",
            "\n",
            "Noisy_monotone Sequence:\n",
            "Score: 1.000\n",
            "Details: {'samples': 139, 'violations': 0, 'checks': 9591, 'epsilon': 0.1, 'sequence_length': 1000}\n",
            "\n",
            "Partially_monotone Sequence:\n",
            "Score: 0.647\n",
            "Details: {'samples': 139, 'violations': 3383, 'checks': 9591, 'epsilon': 0.1, 'sequence_length': 1000}\n",
            "\n",
            "\n",
            "Testing Model Integration:\n",
            "Input shape: torch.Size([32, 10])\n",
            "Scores shape: torch.Size([32])\n",
            "Output shape: torch.Size([32, 1])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from typing import Optional, Union, Tuple\n",
        "\n",
        "# ------------------------------\n",
        "# Input Validation\n",
        "# ------------------------------\n",
        "\n",
        "def validate_inputs(input_tensor: torch.Tensor, epsilon: float) -> None:\n",
        "    \"\"\"\n",
        "    Validate inputs for monotonicity score calculation to ensure robustness.\n",
        "\n",
        "    Purpose: Ensures the input tensor and epsilon parameter meet requirements before\n",
        "    proceeding with computation, preventing runtime errors or illogical results.\n",
        "\n",
        "    Args:\n",
        "        input_tensor (torch.Tensor): The input sequence to evaluate.\n",
        "        epsilon (float): Error tolerance parameter controlling sample size.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the input tensor isn’t a tensor, isn’t 1D, or if epsilon is invalid.\n",
        "\n",
        "    Why: Robust input validation is critical for ML tools to handle diverse use cases\n",
        "    and provide clear error messages to users.\n",
        "    \"\"\"\n",
        "    if not isinstance(input_tensor, torch.Tensor):\n",
        "        raise ValueError(\"Input must be a torch.Tensor\")\n",
        "    if input_tensor.dim() != 1:\n",
        "        raise ValueError(\"Input must be a 1D tensor\")\n",
        "    if not (0 < epsilon <= 1):\n",
        "        raise ValueError(\"Epsilon must be in range (0, 1]\")\n",
        "\n",
        "# ------------------------------\n",
        "# Sample Size Calculation\n",
        "# ------------------------------\n",
        "\n",
        "def calculate_sample_size(n: int, epsilon: float, c: float = 2.0) -> int:\n",
        "    \"\"\"\n",
        "    Calculate the number of samples needed for sublinear computation.\n",
        "\n",
        "    Purpose: Determines how many points to sample based on sequence length and\n",
        "    error tolerance, ensuring sublinear complexity O((1/ε) log n).\n",
        "\n",
        "    Args:\n",
        "        n (int): Length of the input sequence.\n",
        "        epsilon (float): Error tolerance (smaller values increase sample size).\n",
        "        c (float): Sampling constant, default 2.0 (tunable for accuracy vs. speed).\n",
        "\n",
        "    Returns:\n",
        "        int: Number of samples to use, capped at sequence length.\n",
        "\n",
        "    Why: Sublinear sampling reduces computational cost, making this practical for\n",
        "    large datasets in ML preprocessing or feature extraction.\n",
        "    \"\"\"\n",
        "    num_samples = int(math.ceil(c / epsilon * math.log(n)))\n",
        "    return min(num_samples, n)\n",
        "\n",
        "# ------------------------------\n",
        "# Vectorized Violation Counting\n",
        "# ------------------------------\n",
        "\n",
        "def compute_violations_vectorized(values: torch.Tensor, indices: torch.Tensor) -> Tuple[int, int]:\n",
        "    \"\"\"\n",
        "    Compute monotonicity violations using vectorized operations for efficiency.\n",
        "\n",
        "    Purpose: Counts instances where a later value in the sequence is less than an\n",
        "    earlier value (a violation), leveraging PyTorch’s tensor operations.\n",
        "\n",
        "    Args:\n",
        "        values (torch.Tensor): Sampled values from the sequence.\n",
        "        indices (torch.Tensor): Corresponding original indices of sampled values.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[int, int]: Total number of violations and total checks performed.\n",
        "\n",
        "    Why: Vectorized computation replaces loops, speeding up the process significantly\n",
        "    for ML applications where performance is key.\n",
        "    \"\"\"\n",
        "    n = len(values)\n",
        "    # Create comparison matrices\n",
        "    idx_matrix = indices.unsqueeze(0) < indices.unsqueeze(1)  # True where i < j\n",
        "    val_matrix = values.unsqueeze(0) > values.unsqueeze(1)    # True where v_i > v_j\n",
        "\n",
        "    # Count violations (i < j but v_i > v_j)\n",
        "    violations = torch.logical_and(idx_matrix, val_matrix).sum().item()\n",
        "    total_checks = idx_matrix.sum().item()  # Total number of comparisons\n",
        "\n",
        "    return violations, total_checks\n",
        "\n",
        "# ------------------------------\n",
        "# Monotonicity Score Function\n",
        "# ------------------------------\n",
        "\n",
        "def monotonicity_score(\n",
        "    input_tensor: torch.Tensor,\n",
        "    epsilon: float = 0.1,\n",
        "    seed: Optional[int] = None,\n",
        "    return_details: bool = False\n",
        ") -> Union[float, Tuple[float, dict]]:\n",
        "    \"\"\"\n",
        "    Compute a sublinear monotonicity score for a 1D tensor.\n",
        "\n",
        "    Purpose: Quantifies how close a sequence is to being monotonically increasing,\n",
        "    using sublinear sampling inspired by sublinear time algorithms. Useful as a\n",
        "    feature in ML models or for data validation.\n",
        "\n",
        "    Args:\n",
        "        input_tensor (torch.Tensor): 1D tensor representing a sequence.\n",
        "        epsilon (float): Error tolerance (0 < ε ≤ 1), default 0.1.\n",
        "        seed (Optional[int]): Random seed for reproducibility.\n",
        "        return_details (bool): If True, returns additional statistics.\n",
        "\n",
        "    Returns:\n",
        "        Union[float, Tuple[float, dict]]:\n",
        "            - float: Score between 0 (non-monotone) and 1 (monotone) if return_details=False.\n",
        "            - Tuple[float, dict]: (score, details) if return_details=True.\n",
        "\n",
        "    Examples:\n",
        "        >>> seq = torch.tensor([1.0, 2.0, 3.0, 2.5, 4.0])\n",
        "        >>> score = monotonicity_score(seq, epsilon=0.1)\n",
        "        >>> score, details = monotonicity_score(seq, epsilon=0.1, return_details=True)\n",
        "\n",
        "    Why: Offers a fast, scalable way to assess monotonicity, enhancing ML workflows\n",
        "    for time series, tabular data, or ranking tasks.\n",
        "    \"\"\"\n",
        "    # Validate inputs\n",
        "    validate_inputs(input_tensor, epsilon)\n",
        "\n",
        "    n = input_tensor.size(0)\n",
        "    if n < 2:\n",
        "        # Trivially monotone for short sequences\n",
        "        return (1.0, {\"samples\": 0, \"violations\": 0, \"checks\": 0}) if return_details else 1.0\n",
        "\n",
        "    # Set random seed for reproducibility\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "    # Determine sample size\n",
        "    num_samples = calculate_sample_size(n, epsilon)\n",
        "\n",
        "    # Sample and sort indices and values\n",
        "    sampled_indices = torch.randperm(n)[:num_samples]\n",
        "    sampled_values = input_tensor[sampled_indices]\n",
        "    sorted_indices = torch.argsort(sampled_indices)\n",
        "    sampled_indices = sampled_indices[sorted_indices]\n",
        "    sampled_values = sampled_values[sorted_indices]\n",
        "\n",
        "    # Compute violations\n",
        "    total_violations, total_checks = compute_violations_vectorized(sampled_values, sampled_indices)\n",
        "\n",
        "    # Calculate score\n",
        "    score = 1.0 if total_checks == 0 else max(0.0, 1.0 - total_violations / total_checks)\n",
        "\n",
        "    # Return with details if requested\n",
        "    if return_details:\n",
        "        details = {\n",
        "            \"samples\": num_samples,\n",
        "            \"violations\": total_violations,\n",
        "            \"checks\": total_checks,\n",
        "            \"epsilon\": epsilon,\n",
        "            \"sequence_length\": n\n",
        "        }\n",
        "        return score, details\n",
        "\n",
        "    return score\n",
        "\n",
        "# ------------------------------\n",
        "# Enhanced Neural Network Model\n",
        "# ------------------------------\n",
        "\n",
        "class EnhancedModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Enhanced neural network incorporating monotonicity scores as features.\n",
        "\n",
        "    Purpose: Demonstrates integration of the monotonicity score into a deep learning\n",
        "    model, with multiple layers for improved capacity and regularization.\n",
        "\n",
        "    Why: Shows how structural features like monotonicity can enhance predictive\n",
        "    models in ML, especially for tasks where trends matter.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim: int,\n",
        "        hidden_dims: list = [64, 32],\n",
        "        dropout_rate: float = 0.3\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the model with flexible architecture.\n",
        "\n",
        "        Args:\n",
        "            input_dim (int): Number of input features (excluding monotonicity score).\n",
        "            hidden_dims (list): List of hidden layer sizes, default [64, 32].\n",
        "            dropout_rate (float): Dropout probability for regularization, default 0.3.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Dynamically build layers\n",
        "        layers = []\n",
        "        prev_dim = input_dim + 1  # Add 1 for monotonicity score\n",
        "\n",
        "        for hidden_dim in hidden_dims:\n",
        "            layers.extend([\n",
        "                nn.Linear(prev_dim, hidden_dim),\n",
        "                nn.BatchNorm1d(hidden_dim),  # Normalize for stability\n",
        "                nn.ReLU(),                   # Non-linearity\n",
        "                nn.Dropout(dropout_rate)     # Prevent overfitting\n",
        "            ])\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "        layers.append(nn.Linear(prev_dim, 1))  # Output layer\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, mono_scores: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass incorporating monotonicity scores.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, input_dim).\n",
        "            mono_scores (torch.Tensor): Monotonicity scores of shape (batch_size,).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor of shape (batch_size, 1).\n",
        "\n",
        "        Why: Combines raw features with monotonicity information for richer input.\n",
        "        \"\"\"\n",
        "        mono_scores = mono_scores.unsqueeze(1)  # Shape: (batch_size, 1)\n",
        "        x = torch.cat([x, mono_scores], dim=1)  # Shape: (batch_size, input_dim + 1)\n",
        "        return self.network(x)\n",
        "\n",
        "# ------------------------------\n",
        "# Test Sequence Generation\n",
        "# ------------------------------\n",
        "\n",
        "def generate_test_sequences(n: int = 1000, seed: Optional[int] = None) -> dict:\n",
        "    \"\"\"\n",
        "    Generate various test sequences for evaluation.\n",
        "\n",
        "    Purpose: Creates diverse sequences to test the monotonicity score function,\n",
        "    simulating real-world ML data scenarios.\n",
        "\n",
        "    Args:\n",
        "        n (int): Length of sequences, default 1000.\n",
        "        seed (Optional[int]): Random seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary of named test sequences.\n",
        "\n",
        "    Why: Provides a benchmark to verify the function’s behavior across different cases.\n",
        "    \"\"\"\n",
        "    if seed is not None:\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "    return {\n",
        "        \"monotone\": torch.arange(n, dtype=torch.float32),\n",
        "        \"random\": torch.rand(n),\n",
        "        \"decreasing\": torch.flip(torch.arange(n + 1, dtype=torch.float32), dims=[0]),\n",
        "        \"noisy_monotone\": torch.arange(n, dtype=torch.float32) + torch.randn(n) * 0.1,\n",
        "        \"partially_monotone\": torch.cat([\n",
        "            torch.arange(n // 2, dtype=torch.float32),\n",
        "            torch.rand(n - n // 2) * (n // 2)\n",
        "        ])\n",
        "    }\n",
        "\n",
        "# ------------------------------\n",
        "# Main Testing Block\n",
        "# ------------------------------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Generate test sequences\n",
        "    sequences = generate_test_sequences(seed=42)\n",
        "    epsilon = 0.1\n",
        "\n",
        "    # Test monotonicity score function\n",
        "    print(\"\\nTesting Monotonicity Score Function:\")\n",
        "    for name, seq in sequences.items():\n",
        "        score, details = monotonicity_score(seq, epsilon, seed=42, return_details=True)\n",
        "        print(f\"\\n{name.capitalize()} Sequence:\")\n",
        "        print(f\"Score: {score:.3f}\")\n",
        "        print(f\"Details: {details}\")\n",
        "\n",
        "    # Test model integration\n",
        "    print(\"\\n\\nTesting Model Integration:\")\n",
        "    batch_size = 32\n",
        "    seq_length = 10\n",
        "    data = torch.rand(batch_size, seq_length)\n",
        "\n",
        "    # Compute monotonicity scores for each sample\n",
        "    mono_scores = torch.tensor(\n",
        "        [monotonicity_score(data[i], epsilon) for i in range(batch_size)],\n",
        "        dtype=torch.float32\n",
        "    )\n",
        "\n",
        "    # Initialize and run model\n",
        "    model = EnhancedModel(seq_length)\n",
        "    output = model(data, mono_scores)\n",
        "\n",
        "    # Display results\n",
        "    print(f\"Input shape: {data.shape}\")\n",
        "    print(f\"Scores shape: {mono_scores.shape}\")\n",
        "    print(f\"Output shape: {output.shape}\")"
      ]
    }
  ]
}